{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S0UfjwAEY_A"
      },
      "source": [
        "### Importovanje potrebnih lib-ova\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-7WUxwcVdWT",
        "outputId": "dd5496f7-e2a5-4f15-ce33-880d87602f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "try:\n",
        "    import torchinfo\n",
        "except:\n",
        "    !pip install torchinfo\n",
        "    import torchinfo\n",
        "\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fQXIJypEi20"
      },
      "source": [
        "### Setovanje vrednosti za ponovne treninge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LFU3kgHGkQ2k"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17L5KrD_ExOc"
      },
      "source": [
        "### Setovanje komponente na kojoj ce se vrsiti ceo proces\n",
        "GPU ako je dostupan, u suprotnom CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_tUktPBrVjRo",
        "outputId": "9e260daf-a11a-480e-c3c4-113272dcdeb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJu9Tyx1FDI-"
      },
      "source": [
        "### Ucitavanje podataka"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zONxpGuZW4gI",
        "outputId": "1ff51c6e-3ec4-4eaf-c9a0-2495fcf5979b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:05<00:00, 29300781.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL4TzpzKFFVh"
      },
      "source": [
        "### Definisanje metoda za treniranje i testiranje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LLrYS3rPXDKQ"
      },
      "outputs": [],
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    model.train()\n",
        "\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        y_pred = model(X)\n",
        "\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GU63xYAIXHlp"
      },
      "outputs": [],
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module):\n",
        "    model.eval()\n",
        "\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jakyBQyTXIcw"
      },
      "outputs": [],
      "source": [
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
        "          epochs: int = 5):\n",
        "\n",
        "    results = {\"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                           dataloader=train_dataloader,\n",
        "                                           loss_fn=loss_fn,\n",
        "                                           optimizer=optimizer)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYTfuuIsFHXm"
      },
      "source": [
        "### Definisanje konstanti\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L5_ExSMyCBcT"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H00qf_u8FLU8"
      },
      "source": [
        "### Kreiranje glavne klase modela"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JcDIJlHECS0d"
      },
      "outputs": [],
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, in_channels, num_classes):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = conv_block(in_channels, 64)\n",
        "    self.conv2 = conv_block(64, 128, pool=True)\n",
        "    self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "\n",
        "    self.conv3 = conv_block(128, 256, pool=True)\n",
        "    self.conv4 = conv_block(256, 512, pool=True)\n",
        "    self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "    self.conv5 = conv_block(512, 1028, pool=True)\n",
        "    self.res3 = nn.Sequential(conv_block(1028, 1028), conv_block(1028, 1028))\n",
        "\n",
        "    self.classifier = nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n",
        "                                    nn.Flatten(), # 1028\n",
        "                                    nn.Linear(1028, num_classes)) # 1028 -> 100\n",
        "\n",
        "  def forward(self, xb):\n",
        "    out = self.conv1(xb)\n",
        "    out = self.conv2(out)\n",
        "    out = self.res1(out) + out\n",
        "    out = self.conv3(out)\n",
        "    out = self.conv4(out)\n",
        "    out = self.res2(out) + out\n",
        "    out = self.conv5(out)\n",
        "    out = self.res3(out) + out\n",
        "    out = self.classifier(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_X_BgWDFWhC"
      },
      "source": [
        "### Instanciranje modela"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BQOyxqt6sEuV"
      },
      "outputs": [],
      "source": [
        "model = Net(3, 100).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64SdLVEkFY-4"
      },
      "source": [
        "### Pregled modela"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EVZYHIf_0Xfu",
        "outputId": "44d07958-7104-4376-e94f-f8c34a8ba836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "Net                                      --\n",
              "├─Sequential: 1-1                        --\n",
              "│    └─Conv2d: 2-1                       1,792\n",
              "│    └─BatchNorm2d: 2-2                  128\n",
              "│    └─ReLU: 2-3                         --\n",
              "├─Sequential: 1-2                        --\n",
              "│    └─Conv2d: 2-4                       73,856\n",
              "│    └─BatchNorm2d: 2-5                  256\n",
              "│    └─ReLU: 2-6                         --\n",
              "│    └─MaxPool2d: 2-7                    --\n",
              "├─Sequential: 1-3                        --\n",
              "│    └─Sequential: 2-8                   --\n",
              "│    │    └─Conv2d: 3-1                  147,584\n",
              "│    │    └─BatchNorm2d: 3-2             256\n",
              "│    │    └─ReLU: 3-3                    --\n",
              "│    └─Sequential: 2-9                   --\n",
              "│    │    └─Conv2d: 3-4                  147,584\n",
              "│    │    └─BatchNorm2d: 3-5             256\n",
              "│    │    └─ReLU: 3-6                    --\n",
              "├─Sequential: 1-4                        --\n",
              "│    └─Conv2d: 2-10                      295,168\n",
              "│    └─BatchNorm2d: 2-11                 512\n",
              "│    └─ReLU: 2-12                        --\n",
              "│    └─MaxPool2d: 2-13                   --\n",
              "├─Sequential: 1-5                        --\n",
              "│    └─Conv2d: 2-14                      1,180,160\n",
              "│    └─BatchNorm2d: 2-15                 1,024\n",
              "│    └─ReLU: 2-16                        --\n",
              "│    └─MaxPool2d: 2-17                   --\n",
              "├─Sequential: 1-6                        --\n",
              "│    └─Sequential: 2-18                  --\n",
              "│    │    └─Conv2d: 3-7                  2,359,808\n",
              "│    │    └─BatchNorm2d: 3-8             1,024\n",
              "│    │    └─ReLU: 3-9                    --\n",
              "│    └─Sequential: 2-19                  --\n",
              "│    │    └─Conv2d: 3-10                 2,359,808\n",
              "│    │    └─BatchNorm2d: 3-11            1,024\n",
              "│    │    └─ReLU: 3-12                   --\n",
              "├─Sequential: 1-7                        --\n",
              "│    └─Conv2d: 2-20                      4,738,052\n",
              "│    └─BatchNorm2d: 2-21                 2,056\n",
              "│    └─ReLU: 2-22                        --\n",
              "│    └─MaxPool2d: 2-23                   --\n",
              "├─Sequential: 1-8                        --\n",
              "│    └─Sequential: 2-24                  --\n",
              "│    │    └─Conv2d: 3-13                 9,512,084\n",
              "│    │    └─BatchNorm2d: 3-14            2,056\n",
              "│    │    └─ReLU: 3-15                   --\n",
              "│    └─Sequential: 2-25                  --\n",
              "│    │    └─Conv2d: 3-16                 9,512,084\n",
              "│    │    └─BatchNorm2d: 3-17            2,056\n",
              "│    │    └─ReLU: 3-18                   --\n",
              "├─Sequential: 1-9                        --\n",
              "│    └─MaxPool2d: 2-26                   --\n",
              "│    └─Flatten: 2-27                     --\n",
              "│    └─Linear: 2-28                      102,900\n",
              "=================================================================\n",
              "Total params: 30,441,528\n",
              "Trainable params: 30,441,528\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7SBfGsSFc1U"
      },
      "source": [
        "### Treniranje modela i zamrzavanje slojeva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ca20d5fc081b4276ba07b599e3c2bc24",
            "f1d732c0e0a143888a2ee9463a8c9d71",
            "3e267d1f9c3f40f3a9fcfebbbbe61cff",
            "1960c8d560b9420994f791226ba5cf44",
            "abddeae2eaeb4a6da20d6dd771558996",
            "56ab8c2dd7a94a0b80d4907e1cc4cbdb",
            "ae9ddc230eb243edb04133209904351b",
            "cb55d150c3334bb9a7d429309fafa922",
            "443ba28d72bc4601b1d51867ca444603",
            "bf205a682c5844e9939d2eed1d334459",
            "bd7d2f4351bf461581e22025aaccde38"
          ]
        },
        "id": "EsDB1jgNxgm1",
        "outputId": "ef804e87-e185-4ec1-90ad-ec1b31dfe3ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca20d5fc081b4276ba07b599e3c2bc24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 3.0112 | train_acc: 0.2699 | test_loss: 2.8367 | test_acc: 0.3150\n",
            "Epoch: 2 | train_loss: 2.2611 | train_acc: 0.4095 | test_loss: 2.1832 | test_acc: 0.4309\n",
            "Epoch: 3 | train_loss: 1.8759 | train_acc: 0.4919 | test_loss: 1.9715 | test_acc: 0.4944\n",
            "Epoch: 4 | train_loss: 1.6191 | train_acc: 0.5488 | test_loss: 1.7331 | test_acc: 0.5365\n",
            "Epoch: 5 | train_loss: 1.4237 | train_acc: 0.5973 | test_loss: 1.5760 | test_acc: 0.5693\n",
            "Epoch: 6 | train_loss: 1.2570 | train_acc: 0.6404 | test_loss: 1.4762 | test_acc: 0.5879\n",
            "Epoch: 7 | train_loss: 1.1123 | train_acc: 0.6759 | test_loss: 1.4364 | test_acc: 0.6065\n",
            "Epoch: 8 | train_loss: 0.9911 | train_acc: 0.7091 | test_loss: 1.4677 | test_acc: 0.6124\n",
            "Epoch: 9 | train_loss: 0.8805 | train_acc: 0.7367 | test_loss: 1.5236 | test_acc: 0.6058\n",
            "Epoch: 10 | train_loss: 0.7668 | train_acc: 0.7669 | test_loss: 1.4095 | test_acc: 0.6408\n",
            "Epoch: 11 | train_loss: 0.6767 | train_acc: 0.7927 | test_loss: 1.3635 | test_acc: 0.6397\n",
            "Epoch: 12 | train_loss: 0.5941 | train_acc: 0.8147 | test_loss: 1.3580 | test_acc: 0.6568\n",
            "Epoch: 13 | train_loss: 0.5106 | train_acc: 0.8403 | test_loss: 1.3623 | test_acc: 0.6597\n",
            "Epoch: 14 | train_loss: 0.4381 | train_acc: 0.8622 | test_loss: 1.3904 | test_acc: 0.6642\n",
            "Epoch: 15 | train_loss: 0.3881 | train_acc: 0.8762 | test_loss: 1.4275 | test_acc: 0.6689\n",
            "Epoch: 16 | train_loss: 0.3353 | train_acc: 0.8919 | test_loss: 1.4901 | test_acc: 0.6658\n",
            "Epoch: 17 | train_loss: 0.2988 | train_acc: 0.9035 | test_loss: 1.5202 | test_acc: 0.6707\n",
            "Epoch: 18 | train_loss: 0.2660 | train_acc: 0.9151 | test_loss: 1.5221 | test_acc: 0.6701\n",
            "Epoch: 19 | train_loss: 0.2447 | train_acc: 0.9215 | test_loss: 1.5523 | test_acc: 0.6770\n",
            "Epoch: 20 | train_loss: 0.2248 | train_acc: 0.9279 | test_loss: 1.6908 | test_acc: 0.6651\n",
            "Epoch: 21 | train_loss: 0.1985 | train_acc: 0.9367 | test_loss: 1.5477 | test_acc: 0.6822\n",
            "Epoch: 22 | train_loss: 0.1838 | train_acc: 0.9404 | test_loss: 1.7135 | test_acc: 0.6670\n",
            "Epoch: 23 | train_loss: 0.1790 | train_acc: 0.9423 | test_loss: 1.6921 | test_acc: 0.6711\n",
            "Epoch: 24 | train_loss: 0.1647 | train_acc: 0.9461 | test_loss: 1.6936 | test_acc: 0.6760\n",
            "Epoch: 25 | train_loss: 0.1463 | train_acc: 0.9523 | test_loss: 1.7355 | test_acc: 0.6763\n",
            "Epoch: 26 | train_loss: 0.1496 | train_acc: 0.9519 | test_loss: 1.7523 | test_acc: 0.6791\n",
            "Epoch: 27 | train_loss: 0.1516 | train_acc: 0.9502 | test_loss: 1.8038 | test_acc: 0.6744\n",
            "Epoch: 28 | train_loss: 0.1359 | train_acc: 0.9562 | test_loss: 1.8354 | test_acc: 0.6790\n",
            "Epoch: 29 | train_loss: 0.1239 | train_acc: 0.9589 | test_loss: 1.8960 | test_acc: 0.6715\n",
            "Epoch: 30 | train_loss: 0.1224 | train_acc: 0.9604 | test_loss: 1.8424 | test_acc: 0.6724\n",
            "Epoch: 31 | train_loss: 0.1142 | train_acc: 0.9629 | test_loss: 1.8051 | test_acc: 0.6827\n",
            "Epoch: 32 | train_loss: 0.1107 | train_acc: 0.9643 | test_loss: 1.9011 | test_acc: 0.6742\n",
            "Epoch: 33 | train_loss: 0.1098 | train_acc: 0.9647 | test_loss: 1.8623 | test_acc: 0.6800\n",
            "Epoch: 34 | train_loss: 0.1053 | train_acc: 0.9660 | test_loss: 1.8430 | test_acc: 0.6829\n",
            "Epoch: 35 | train_loss: 0.1046 | train_acc: 0.9657 | test_loss: 1.8773 | test_acc: 0.6803\n",
            "Epoch: 36 | train_loss: 0.0953 | train_acc: 0.9692 | test_loss: 1.8859 | test_acc: 0.6831\n",
            "Epoch: 37 | train_loss: 0.0930 | train_acc: 0.9698 | test_loss: 1.9511 | test_acc: 0.6827\n",
            "Epoch: 38 | train_loss: 0.0932 | train_acc: 0.9702 | test_loss: 2.0304 | test_acc: 0.6708\n",
            "Epoch: 39 | train_loss: 0.0947 | train_acc: 0.9696 | test_loss: 2.0108 | test_acc: 0.6817\n",
            "Epoch: 40 | train_loss: 0.0866 | train_acc: 0.9725 | test_loss: 1.9147 | test_acc: 0.6798\n",
            "Epoch: 41 | train_loss: 0.0810 | train_acc: 0.9735 | test_loss: 2.0564 | test_acc: 0.6771\n",
            "Epoch: 42 | train_loss: 0.0873 | train_acc: 0.9717 | test_loss: 1.9981 | test_acc: 0.6799\n",
            "Epoch: 43 | train_loss: 0.0841 | train_acc: 0.9721 | test_loss: 2.0015 | test_acc: 0.6855\n",
            "Epoch: 44 | train_loss: 0.0785 | train_acc: 0.9753 | test_loss: 2.0225 | test_acc: 0.6827\n",
            "Epoch: 45 | train_loss: 0.0790 | train_acc: 0.9747 | test_loss: 2.0732 | test_acc: 0.6915\n",
            "Epoch: 46 | train_loss: 0.0723 | train_acc: 0.9765 | test_loss: 2.0195 | test_acc: 0.6853\n",
            "Epoch: 47 | train_loss: 0.0725 | train_acc: 0.9766 | test_loss: 2.1234 | test_acc: 0.6853\n",
            "Epoch: 48 | train_loss: 0.0767 | train_acc: 0.9748 | test_loss: 2.1045 | test_acc: 0.6892\n",
            "Epoch: 49 | train_loss: 0.0711 | train_acc: 0.9774 | test_loss: 2.1673 | test_acc: 0.6796\n",
            "Epoch: 50 | train_loss: 0.0684 | train_acc: 0.9781 | test_loss: 2.1015 | test_acc: 0.6921\n",
            "Epoch: 51 | train_loss: 0.0705 | train_acc: 0.9771 | test_loss: 2.0993 | test_acc: 0.6901\n",
            "Epoch: 52 | train_loss: 0.0655 | train_acc: 0.9785 | test_loss: 2.1186 | test_acc: 0.6837\n",
            "Epoch: 53 | train_loss: 0.0741 | train_acc: 0.9760 | test_loss: 2.0961 | test_acc: 0.6903\n",
            "Epoch: 54 | train_loss: 0.0588 | train_acc: 0.9812 | test_loss: 2.1300 | test_acc: 0.6845\n",
            "Epoch: 55 | train_loss: 0.0653 | train_acc: 0.9789 | test_loss: 2.1423 | test_acc: 0.6888\n",
            "Epoch: 56 | train_loss: 0.0650 | train_acc: 0.9792 | test_loss: 2.1457 | test_acc: 0.6921\n",
            "Epoch: 57 | train_loss: 0.0660 | train_acc: 0.9788 | test_loss: 2.1025 | test_acc: 0.6918\n",
            "Epoch: 58 | train_loss: 0.0536 | train_acc: 0.9828 | test_loss: 2.2340 | test_acc: 0.6799\n",
            "Epoch: 59 | train_loss: 0.0620 | train_acc: 0.9796 | test_loss: 2.3244 | test_acc: 0.6744\n",
            "Epoch: 60 | train_loss: 0.0588 | train_acc: 0.9815 | test_loss: 2.1809 | test_acc: 0.6919\n",
            "Epoch: 61 | train_loss: 0.0575 | train_acc: 0.9824 | test_loss: 2.1954 | test_acc: 0.6873\n",
            "Epoch: 62 | train_loss: 0.0623 | train_acc: 0.9799 | test_loss: 2.2328 | test_acc: 0.6837\n",
            "Epoch: 63 | train_loss: 0.0551 | train_acc: 0.9826 | test_loss: 2.1441 | test_acc: 0.6887\n",
            "Epoch: 64 | train_loss: 0.0562 | train_acc: 0.9821 | test_loss: 2.1257 | test_acc: 0.6951\n",
            "Epoch: 65 | train_loss: 0.0593 | train_acc: 0.9809 | test_loss: 2.2307 | test_acc: 0.6944\n",
            "Epoch: 66 | train_loss: 0.0512 | train_acc: 0.9837 | test_loss: 2.2175 | test_acc: 0.6935\n",
            "Epoch: 67 | train_loss: 0.0490 | train_acc: 0.9845 | test_loss: 2.2023 | test_acc: 0.6914\n",
            "Epoch: 68 | train_loss: 0.0517 | train_acc: 0.9832 | test_loss: 2.2355 | test_acc: 0.6892\n",
            "Epoch: 69 | train_loss: 0.0557 | train_acc: 0.9821 | test_loss: 2.1346 | test_acc: 0.6950\n",
            "Epoch: 70 | train_loss: 0.0513 | train_acc: 0.9843 | test_loss: 2.3153 | test_acc: 0.6777\n",
            "Epoch: 71 | train_loss: 0.0503 | train_acc: 0.9843 | test_loss: 2.2199 | test_acc: 0.6923\n",
            "Epoch: 72 | train_loss: 0.0501 | train_acc: 0.9839 | test_loss: 2.2928 | test_acc: 0.6854\n",
            "Epoch: 73 | train_loss: 0.0505 | train_acc: 0.9846 | test_loss: 2.2215 | test_acc: 0.6952\n",
            "Epoch: 74 | train_loss: 0.0458 | train_acc: 0.9851 | test_loss: 2.2988 | test_acc: 0.6901\n",
            "Epoch: 75 | train_loss: 0.0492 | train_acc: 0.9846 | test_loss: 2.2799 | test_acc: 0.6898\n",
            "Epoch: 76 | train_loss: 0.0459 | train_acc: 0.9855 | test_loss: 2.3256 | test_acc: 0.6888\n",
            "Epoch: 77 | train_loss: 0.0492 | train_acc: 0.9844 | test_loss: 2.4522 | test_acc: 0.6783\n",
            "Epoch: 78 | train_loss: 0.0445 | train_acc: 0.9849 | test_loss: 2.2785 | test_acc: 0.6867\n",
            "Epoch: 79 | train_loss: 0.0429 | train_acc: 0.9863 | test_loss: 2.2726 | test_acc: 0.6927\n",
            "Epoch: 80 | train_loss: 0.0421 | train_acc: 0.9866 | test_loss: 2.3531 | test_acc: 0.6904\n",
            "Epoch: 81 | train_loss: 0.0442 | train_acc: 0.9858 | test_loss: 2.3269 | test_acc: 0.6892\n",
            "Epoch: 82 | train_loss: 0.0426 | train_acc: 0.9860 | test_loss: 2.3538 | test_acc: 0.6912\n",
            "Epoch: 83 | train_loss: 0.0460 | train_acc: 0.9854 | test_loss: 2.3701 | test_acc: 0.6892\n",
            "Epoch: 84 | train_loss: 0.0471 | train_acc: 0.9856 | test_loss: 2.3186 | test_acc: 0.6904\n",
            "Epoch: 85 | train_loss: 0.0390 | train_acc: 0.9871 | test_loss: 2.2956 | test_acc: 0.6880\n",
            "Epoch: 86 | train_loss: 0.0424 | train_acc: 0.9867 | test_loss: 2.2922 | test_acc: 0.6948\n",
            "Epoch: 87 | train_loss: 0.0461 | train_acc: 0.9851 | test_loss: 2.5302 | test_acc: 0.6760\n",
            "Epoch: 88 | train_loss: 0.0387 | train_acc: 0.9879 | test_loss: 2.3191 | test_acc: 0.6906\n",
            "Epoch: 89 | train_loss: 0.0380 | train_acc: 0.9887 | test_loss: 2.3987 | test_acc: 0.6854\n",
            "Epoch: 90 | train_loss: 0.0397 | train_acc: 0.9871 | test_loss: 2.3891 | test_acc: 0.6902\n",
            "Epoch: 91 | train_loss: 0.0433 | train_acc: 0.9863 | test_loss: 2.4191 | test_acc: 0.6941\n",
            "Epoch: 92 | train_loss: 0.0397 | train_acc: 0.9872 | test_loss: 2.4795 | test_acc: 0.6866\n",
            "Epoch: 93 | train_loss: 0.0393 | train_acc: 0.9870 | test_loss: 2.4478 | test_acc: 0.6917\n",
            "Epoch: 94 | train_loss: 0.0394 | train_acc: 0.9875 | test_loss: 2.4438 | test_acc: 0.6901\n",
            "Epoch: 95 | train_loss: 0.0367 | train_acc: 0.9883 | test_loss: 2.4614 | test_acc: 0.6955\n",
            "Epoch: 96 | train_loss: 0.0436 | train_acc: 0.9865 | test_loss: 2.4278 | test_acc: 0.6945\n",
            "Epoch: 97 | train_loss: 0.0389 | train_acc: 0.9882 | test_loss: 2.4555 | test_acc: 0.6890\n",
            "Epoch: 98 | train_loss: 0.0379 | train_acc: 0.9878 | test_loss: 2.5805 | test_acc: 0.6795\n",
            "Epoch: 99 | train_loss: 0.0382 | train_acc: 0.9874 | test_loss: 2.4189 | test_acc: 0.6909\n",
            "Epoch: 100 | train_loss: 0.0402 | train_acc: 0.9879 | test_loss: 2.3855 | test_acc: 0.6928\n",
            "Epoch: 101 | train_loss: 0.0350 | train_acc: 0.9890 | test_loss: 2.5130 | test_acc: 0.6825\n",
            "Epoch: 102 | train_loss: 0.0343 | train_acc: 0.9890 | test_loss: 2.5449 | test_acc: 0.6867\n",
            "Epoch: 103 | train_loss: 0.0391 | train_acc: 0.9874 | test_loss: 2.4751 | test_acc: 0.6908\n",
            "Epoch: 104 | train_loss: 0.0346 | train_acc: 0.9888 | test_loss: 2.5768 | test_acc: 0.6870\n",
            "Epoch: 105 | train_loss: 0.0344 | train_acc: 0.9892 | test_loss: 2.5354 | test_acc: 0.6890\n",
            "Epoch: 106 | train_loss: 0.0367 | train_acc: 0.9878 | test_loss: 2.5513 | test_acc: 0.6914\n",
            "Epoch: 107 | train_loss: 0.0337 | train_acc: 0.9894 | test_loss: 2.4513 | test_acc: 0.6926\n",
            "Epoch: 108 | train_loss: 0.0385 | train_acc: 0.9881 | test_loss: 2.5811 | test_acc: 0.6894\n",
            "Epoch: 109 | train_loss: 0.0346 | train_acc: 0.9892 | test_loss: 2.4814 | test_acc: 0.6996\n",
            "Epoch: 110 | train_loss: 0.0334 | train_acc: 0.9898 | test_loss: 2.4840 | test_acc: 0.6963\n",
            "Epoch: 111 | train_loss: 0.0347 | train_acc: 0.9889 | test_loss: 2.5167 | test_acc: 0.6948\n",
            "Epoch: 112 | train_loss: 0.0346 | train_acc: 0.9891 | test_loss: 2.6379 | test_acc: 0.6919\n",
            "Epoch: 113 | train_loss: 0.0362 | train_acc: 0.9884 | test_loss: 2.5056 | test_acc: 0.6963\n",
            "Epoch: 114 | train_loss: 0.0323 | train_acc: 0.9896 | test_loss: 2.5240 | test_acc: 0.6904\n",
            "Epoch: 115 | train_loss: 0.0325 | train_acc: 0.9892 | test_loss: 2.5684 | test_acc: 0.6933\n",
            "Epoch: 116 | train_loss: 0.0308 | train_acc: 0.9903 | test_loss: 2.5803 | test_acc: 0.6904\n",
            "Epoch: 117 | train_loss: 0.0355 | train_acc: 0.9889 | test_loss: 2.4970 | test_acc: 0.6973\n",
            "Epoch: 118 | train_loss: 0.0312 | train_acc: 0.9897 | test_loss: 2.6709 | test_acc: 0.6959\n",
            "Epoch: 119 | train_loss: 0.0310 | train_acc: 0.9901 | test_loss: 2.6031 | test_acc: 0.6879\n",
            "Epoch: 120 | train_loss: 0.0320 | train_acc: 0.9899 | test_loss: 2.6435 | test_acc: 0.6932\n",
            "Epoch: 121 | train_loss: 0.0348 | train_acc: 0.9892 | test_loss: 2.6399 | test_acc: 0.6940\n",
            "Epoch: 122 | train_loss: 0.0300 | train_acc: 0.9905 | test_loss: 2.6002 | test_acc: 0.6981\n",
            "Epoch: 123 | train_loss: 0.0300 | train_acc: 0.9905 | test_loss: 2.6388 | test_acc: 0.6925\n",
            "Epoch: 124 | train_loss: 0.0292 | train_acc: 0.9908 | test_loss: 2.5211 | test_acc: 0.6989\n",
            "Epoch: 125 | train_loss: 0.0319 | train_acc: 0.9902 | test_loss: 2.6082 | test_acc: 0.6950\n",
            "Epoch: 126 | train_loss: 0.0336 | train_acc: 0.9890 | test_loss: 2.5480 | test_acc: 0.6984\n",
            "Epoch: 127 | train_loss: 0.0291 | train_acc: 0.9910 | test_loss: 2.5246 | test_acc: 0.6946\n",
            "Epoch: 128 | train_loss: 0.0301 | train_acc: 0.9905 | test_loss: 2.5729 | test_acc: 0.6966\n",
            "Epoch: 129 | train_loss: 0.0279 | train_acc: 0.9916 | test_loss: 2.5214 | test_acc: 0.7036\n",
            "Epoch: 130 | train_loss: 0.0314 | train_acc: 0.9901 | test_loss: 2.5848 | test_acc: 0.6985\n",
            "Epoch: 131 | train_loss: 0.0302 | train_acc: 0.9902 | test_loss: 2.6072 | test_acc: 0.6960\n",
            "Epoch: 132 | train_loss: 0.0307 | train_acc: 0.9902 | test_loss: 2.5605 | test_acc: 0.6981\n",
            "Epoch: 133 | train_loss: 0.0271 | train_acc: 0.9912 | test_loss: 2.6776 | test_acc: 0.6940\n",
            "Epoch: 134 | train_loss: 0.0319 | train_acc: 0.9899 | test_loss: 2.6154 | test_acc: 0.7002\n",
            "Epoch: 135 | train_loss: 0.0301 | train_acc: 0.9901 | test_loss: 2.6392 | test_acc: 0.6991\n",
            "Epoch: 136 | train_loss: 0.0282 | train_acc: 0.9912 | test_loss: 2.6168 | test_acc: 0.6965\n",
            "Epoch: 137 | train_loss: 0.0294 | train_acc: 0.9910 | test_loss: 2.5842 | test_acc: 0.7018\n",
            "Epoch: 138 | train_loss: 0.0278 | train_acc: 0.9914 | test_loss: 2.6440 | test_acc: 0.6984\n",
            "Epoch: 139 | train_loss: 0.0279 | train_acc: 0.9911 | test_loss: 2.6424 | test_acc: 0.7001\n",
            "Epoch: 140 | train_loss: 0.0292 | train_acc: 0.9909 | test_loss: 2.6069 | test_acc: 0.6946\n",
            "Epoch: 141 | train_loss: 0.0280 | train_acc: 0.9914 | test_loss: 2.5932 | test_acc: 0.6965\n",
            "Epoch: 142 | train_loss: 0.0267 | train_acc: 0.9913 | test_loss: 2.6099 | test_acc: 0.6949\n",
            "Epoch: 143 | train_loss: 0.0233 | train_acc: 0.9927 | test_loss: 2.6258 | test_acc: 0.6978\n",
            "Epoch: 144 | train_loss: 0.0297 | train_acc: 0.9908 | test_loss: 2.7754 | test_acc: 0.6950\n",
            "Epoch: 145 | train_loss: 0.0270 | train_acc: 0.9911 | test_loss: 2.5892 | test_acc: 0.7031\n",
            "Epoch: 146 | train_loss: 0.0260 | train_acc: 0.9917 | test_loss: 2.7943 | test_acc: 0.6915\n",
            "Epoch: 147 | train_loss: 0.0328 | train_acc: 0.9894 | test_loss: 2.7064 | test_acc: 0.6958\n",
            "Epoch: 148 | train_loss: 0.0282 | train_acc: 0.9909 | test_loss: 2.7388 | test_acc: 0.6917\n",
            "Epoch: 149 | train_loss: 0.0288 | train_acc: 0.9916 | test_loss: 2.7802 | test_acc: 0.6922\n",
            "Epoch: 150 | train_loss: 0.0276 | train_acc: 0.9911 | test_loss: 2.7058 | test_acc: 0.6957\n",
            "Epoch: 151 | train_loss: 0.0279 | train_acc: 0.9912 | test_loss: 2.6749 | test_acc: 0.7015\n",
            "Epoch: 152 | train_loss: 0.0243 | train_acc: 0.9920 | test_loss: 2.6908 | test_acc: 0.6987\n",
            "Epoch: 153 | train_loss: 0.0282 | train_acc: 0.9912 | test_loss: 2.6131 | test_acc: 0.6996\n",
            "Epoch: 154 | train_loss: 0.0286 | train_acc: 0.9908 | test_loss: 2.6376 | test_acc: 0.7008\n",
            "Epoch: 155 | train_loss: 0.0242 | train_acc: 0.9919 | test_loss: 2.8062 | test_acc: 0.6929\n",
            "Epoch: 156 | train_loss: 0.0236 | train_acc: 0.9928 | test_loss: 2.6985 | test_acc: 0.7048\n",
            "Epoch: 157 | train_loss: 0.0301 | train_acc: 0.9909 | test_loss: 2.7360 | test_acc: 0.6947\n",
            "Epoch: 158 | train_loss: 0.0275 | train_acc: 0.9909 | test_loss: 2.7428 | test_acc: 0.6909\n",
            "Epoch: 159 | train_loss: 0.0251 | train_acc: 0.9925 | test_loss: 2.6928 | test_acc: 0.6980\n",
            "Epoch: 160 | train_loss: 0.0229 | train_acc: 0.9923 | test_loss: 2.7574 | test_acc: 0.6979\n",
            "Epoch: 161 | train_loss: 0.0238 | train_acc: 0.9926 | test_loss: 2.6966 | test_acc: 0.6987\n",
            "Epoch: 162 | train_loss: 0.0249 | train_acc: 0.9922 | test_loss: 2.8961 | test_acc: 0.6969\n",
            "Epoch: 163 | train_loss: 0.0259 | train_acc: 0.9916 | test_loss: 2.7573 | test_acc: 0.6962\n",
            "Epoch: 164 | train_loss: 0.0264 | train_acc: 0.9908 | test_loss: 2.8613 | test_acc: 0.6939\n",
            "Epoch: 165 | train_loss: 0.0225 | train_acc: 0.9926 | test_loss: 2.7492 | test_acc: 0.7010\n",
            "Epoch: 166 | train_loss: 0.0259 | train_acc: 0.9918 | test_loss: 2.7987 | test_acc: 0.7038\n",
            "Epoch: 167 | train_loss: 0.0257 | train_acc: 0.9919 | test_loss: 2.8137 | test_acc: 0.6988\n",
            "Epoch: 168 | train_loss: 0.0250 | train_acc: 0.9920 | test_loss: 2.8617 | test_acc: 0.6918\n",
            "Epoch: 169 | train_loss: 0.0211 | train_acc: 0.9933 | test_loss: 2.7858 | test_acc: 0.6903\n",
            "Epoch: 170 | train_loss: 0.0251 | train_acc: 0.9919 | test_loss: 2.7946 | test_acc: 0.6986\n",
            "Epoch: 171 | train_loss: 0.0223 | train_acc: 0.9930 | test_loss: 2.7390 | test_acc: 0.6984\n",
            "Epoch: 172 | train_loss: 0.0258 | train_acc: 0.9918 | test_loss: 2.7896 | test_acc: 0.6953\n",
            "Epoch: 173 | train_loss: 0.0258 | train_acc: 0.9919 | test_loss: 2.8293 | test_acc: 0.6981\n",
            "Epoch: 174 | train_loss: 0.0229 | train_acc: 0.9927 | test_loss: 2.8495 | test_acc: 0.6953\n",
            "Epoch: 175 | train_loss: 0.0241 | train_acc: 0.9926 | test_loss: 2.8326 | test_acc: 0.6950\n",
            "Epoch: 176 | train_loss: 0.0254 | train_acc: 0.9918 | test_loss: 2.7776 | test_acc: 0.6980\n",
            "Epoch: 177 | train_loss: 0.0227 | train_acc: 0.9928 | test_loss: 2.7687 | test_acc: 0.6996\n",
            "Epoch: 178 | train_loss: 0.0232 | train_acc: 0.9927 | test_loss: 2.8047 | test_acc: 0.6983\n",
            "Epoch: 179 | train_loss: 0.0223 | train_acc: 0.9929 | test_loss: 2.8514 | test_acc: 0.6978\n",
            "Epoch: 180 | train_loss: 0.0243 | train_acc: 0.9921 | test_loss: 2.9297 | test_acc: 0.6907\n",
            "Epoch: 181 | train_loss: 0.0238 | train_acc: 0.9927 | test_loss: 2.8853 | test_acc: 0.6954\n",
            "Epoch: 182 | train_loss: 0.0225 | train_acc: 0.9925 | test_loss: 2.9337 | test_acc: 0.6933\n",
            "Epoch: 183 | train_loss: 0.0234 | train_acc: 0.9926 | test_loss: 2.8529 | test_acc: 0.6984\n",
            "Epoch: 184 | train_loss: 0.0246 | train_acc: 0.9924 | test_loss: 2.8650 | test_acc: 0.7042\n",
            "Epoch: 185 | train_loss: 0.0204 | train_acc: 0.9937 | test_loss: 2.8663 | test_acc: 0.6984\n",
            "Epoch: 186 | train_loss: 0.0259 | train_acc: 0.9917 | test_loss: 2.8386 | test_acc: 0.6959\n",
            "Epoch: 187 | train_loss: 0.0246 | train_acc: 0.9919 | test_loss: 2.8431 | test_acc: 0.7018\n",
            "Epoch: 188 | train_loss: 0.0208 | train_acc: 0.9937 | test_loss: 2.7826 | test_acc: 0.7042\n",
            "Epoch: 189 | train_loss: 0.0220 | train_acc: 0.9929 | test_loss: 2.9541 | test_acc: 0.6969\n",
            "Epoch: 190 | train_loss: 0.0215 | train_acc: 0.9932 | test_loss: 2.9610 | test_acc: 0.6906\n",
            "Epoch: 191 | train_loss: 0.0245 | train_acc: 0.9922 | test_loss: 2.8239 | test_acc: 0.6972\n",
            "Epoch: 192 | train_loss: 0.0234 | train_acc: 0.9921 | test_loss: 2.8689 | test_acc: 0.7025\n",
            "Epoch: 193 | train_loss: 0.0219 | train_acc: 0.9930 | test_loss: 2.8833 | test_acc: 0.6990\n",
            "Epoch: 194 | train_loss: 0.0249 | train_acc: 0.9923 | test_loss: 2.9112 | test_acc: 0.6995\n",
            "Epoch: 195 | train_loss: 0.0213 | train_acc: 0.9932 | test_loss: 2.8519 | test_acc: 0.7008\n",
            "Epoch: 196 | train_loss: 0.0217 | train_acc: 0.9935 | test_loss: 2.8911 | test_acc: 0.7042\n",
            "Epoch: 197 | train_loss: 0.0227 | train_acc: 0.9929 | test_loss: 2.9406 | test_acc: 0.6979\n",
            "Epoch: 198 | train_loss: 0.0224 | train_acc: 0.9932 | test_loss: 2.8798 | test_acc: 0.7016\n",
            "Epoch: 199 | train_loss: 0.0256 | train_acc: 0.9923 | test_loss: 2.8575 | test_acc: 0.6995\n",
            "Epoch: 200 | train_loss: 0.0228 | train_acc: 0.9924 | test_loss: 2.8921 | test_acc: 0.6979\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_loss': [3.0112396899391625,\n",
              "  2.2611166630559567,\n",
              "  1.8759187767877603,\n",
              "  1.6191028690399112,\n",
              "  1.4237250268001995,\n",
              "  1.257048896968822,\n",
              "  1.1122798710070607,\n",
              "  0.9910725257585725,\n",
              "  0.8805048835780614,\n",
              "  0.7668357771604567,\n",
              "  0.6767172904499351,\n",
              "  0.5941123332818756,\n",
              "  0.5106042162193667,\n",
              "  0.4381349449381804,\n",
              "  0.3880775520944839,\n",
              "  0.33530655927251063,\n",
              "  0.2987882794092988,\n",
              "  0.2660028118344829,\n",
              "  0.24465557001531124,\n",
              "  0.22483866678936706,\n",
              "  0.19845710200784,\n",
              "  0.18377035292213226,\n",
              "  0.1790098868110372,\n",
              "  0.16472555188428792,\n",
              "  0.1463459845384597,\n",
              "  0.14959543441777187,\n",
              "  0.15158495232415245,\n",
              "  0.13585439194803653,\n",
              "  0.12389094273552603,\n",
              "  0.12239718908215384,\n",
              "  0.11424110117344581,\n",
              "  0.11065837781509513,\n",
              "  0.10983971009731217,\n",
              "  0.10528351921383339,\n",
              "  0.10458574932999909,\n",
              "  0.09530185665244527,\n",
              "  0.09298088918428135,\n",
              "  0.09320977576972579,\n",
              "  0.09474494163890171,\n",
              "  0.08663890000237413,\n",
              "  0.08099026675484931,\n",
              "  0.08729112753108659,\n",
              "  0.0840677348354979,\n",
              "  0.07849443629574593,\n",
              "  0.07895067247086922,\n",
              "  0.07231158367655885,\n",
              "  0.07250191526585127,\n",
              "  0.07671063006891275,\n",
              "  0.07107584141409191,\n",
              "  0.06841141383057636,\n",
              "  0.07049538377348495,\n",
              "  0.06554633393988508,\n",
              "  0.07407325971067724,\n",
              "  0.05881018258447823,\n",
              "  0.0652777289833559,\n",
              "  0.06501302362420856,\n",
              "  0.06601055412624113,\n",
              "  0.05362880970288873,\n",
              "  0.06195191035905848,\n",
              "  0.05879996056237337,\n",
              "  0.057490215108752525,\n",
              "  0.06229705199160997,\n",
              "  0.05505531045811875,\n",
              "  0.05619149478664503,\n",
              "  0.05930417222613969,\n",
              "  0.05123737517482591,\n",
              "  0.04904983584534453,\n",
              "  0.051716287686909924,\n",
              "  0.055673752047875995,\n",
              "  0.05131910382079251,\n",
              "  0.05028734178311499,\n",
              "  0.050105115202034624,\n",
              "  0.05052629353583057,\n",
              "  0.04583670931187627,\n",
              "  0.0492291597591694,\n",
              "  0.04585506100385614,\n",
              "  0.04921189728094732,\n",
              "  0.04452812480171571,\n",
              "  0.042907733569645785,\n",
              "  0.042086761000880595,\n",
              "  0.04422955818427697,\n",
              "  0.042557561657387916,\n",
              "  0.04600567672945931,\n",
              "  0.04708890289501728,\n",
              "  0.03896188788877293,\n",
              "  0.04235264405352421,\n",
              "  0.04605190835528723,\n",
              "  0.03872974715171777,\n",
              "  0.03800831621048598,\n",
              "  0.03971722675602772,\n",
              "  0.043326322744955376,\n",
              "  0.03971543504588265,\n",
              "  0.03928555000946015,\n",
              "  0.03941276919032248,\n",
              "  0.03668851096226531,\n",
              "  0.043587029355813635,\n",
              "  0.038899497853715895,\n",
              "  0.0379425385968609,\n",
              "  0.038164208506365295,\n",
              "  0.040197574157405366,\n",
              "  0.03501520149392353,\n",
              "  0.034280072365802015,\n",
              "  0.039145579419644846,\n",
              "  0.034627778734073945,\n",
              "  0.03442611276139946,\n",
              "  0.036654218940337935,\n",
              "  0.03374865654785403,\n",
              "  0.038494036305243805,\n",
              "  0.034615805073109185,\n",
              "  0.03337295936205668,\n",
              "  0.034694419694518676,\n",
              "  0.03455033086505575,\n",
              "  0.03623722173651541,\n",
              "  0.032274074781530646,\n",
              "  0.03245627424694062,\n",
              "  0.030826980612021746,\n",
              "  0.03553636224688905,\n",
              "  0.031226411587438104,\n",
              "  0.0309751293840571,\n",
              "  0.03196817614140955,\n",
              "  0.03477472642498702,\n",
              "  0.029983931990049166,\n",
              "  0.029979429933142163,\n",
              "  0.029206248421413714,\n",
              "  0.031890886581290794,\n",
              "  0.033559904968486015,\n",
              "  0.02909379373696815,\n",
              "  0.030136980682942995,\n",
              "  0.02791659634878851,\n",
              "  0.03138371924940243,\n",
              "  0.030217451022315548,\n",
              "  0.030706264813916813,\n",
              "  0.027069670358852348,\n",
              "  0.031906754693479096,\n",
              "  0.03013602739613181,\n",
              "  0.02822785533915805,\n",
              "  0.029429583512476612,\n",
              "  0.027794467592000802,\n",
              "  0.027894007206085057,\n",
              "  0.029240671996143868,\n",
              "  0.027950682195087048,\n",
              "  0.0266742252591139,\n",
              "  0.02328439963196655,\n",
              "  0.029671894223027316,\n",
              "  0.027009082863142606,\n",
              "  0.02596526196452535,\n",
              "  0.03279666417316534,\n",
              "  0.028238271325583243,\n",
              "  0.028776150854241386,\n",
              "  0.027600713689686328,\n",
              "  0.027861909089644817,\n",
              "  0.024336352089970106,\n",
              "  0.02819976818294811,\n",
              "  0.02859602603997296,\n",
              "  0.024160150727175627,\n",
              "  0.023648571824487312,\n",
              "  0.03014649126360573,\n",
              "  0.02751596704961451,\n",
              "  0.025069107658566882,\n",
              "  0.022929375086336166,\n",
              "  0.02377700068777964,\n",
              "  0.024860345338148338,\n",
              "  0.02589779440064688,\n",
              "  0.026374106627760838,\n",
              "  0.022547876574810655,\n",
              "  0.025864090216635636,\n",
              "  0.025653428815770374,\n",
              "  0.025044652214545497,\n",
              "  0.02114582847840825,\n",
              "  0.02505123793189039,\n",
              "  0.02233467501260431,\n",
              "  0.025770256525853522,\n",
              "  0.025773359017222292,\n",
              "  0.022907993643308436,\n",
              "  0.024089031892199412,\n",
              "  0.02537394852972093,\n",
              "  0.02270546205757448,\n",
              "  0.023240751990200238,\n",
              "  0.02233753769235049,\n",
              "  0.024275139946776877,\n",
              "  0.023754701686902756,\n",
              "  0.022543897109930617,\n",
              "  0.02344403884339291,\n",
              "  0.02464501311968449,\n",
              "  0.020396563501892666,\n",
              "  0.025904028165793402,\n",
              "  0.024615237789434072,\n",
              "  0.020832576242888243,\n",
              "  0.021985871721818463,\n",
              "  0.021546825011592097,\n",
              "  0.024513461184516715,\n",
              "  0.023448912968944007,\n",
              "  0.021882380318782597,\n",
              "  0.02490287351845655,\n",
              "  0.021304133039631915,\n",
              "  0.021746635663803877,\n",
              "  0.022740457344319127,\n",
              "  0.022422443376398152,\n",
              "  0.02557512073512011,\n",
              "  0.02282940942389236],\n",
              " 'train_acc': [0.26988091432225064,\n",
              "  0.40954683503836314,\n",
              "  0.49194773017902815,\n",
              "  0.5488331202046036,\n",
              "  0.5972666240409207,\n",
              "  0.6403852301790282,\n",
              "  0.6759111253196931,\n",
              "  0.7090593030690537,\n",
              "  0.7367127557544757,\n",
              "  0.7669437340153452,\n",
              "  0.7926790281329923,\n",
              "  0.8147178708439897,\n",
              "  0.8402733375959079,\n",
              "  0.8621523337595908,\n",
              "  0.8761588874680307,\n",
              "  0.8918837915601023,\n",
              "  0.9034926470588235,\n",
              "  0.9151214833759591,\n",
              "  0.9215353260869565,\n",
              "  0.9279291879795396,\n",
              "  0.9367007672634271,\n",
              "  0.940417199488491,\n",
              "  0.9423153772378516,\n",
              "  0.9460517902813299,\n",
              "  0.9523457480818415,\n",
              "  0.951886189258312,\n",
              "  0.9502277813299232,\n",
              "  0.9562020460358056,\n",
              "  0.9589394181585678,\n",
              "  0.9603580562659847,\n",
              "  0.9629156010230179,\n",
              "  0.9642543158567775,\n",
              "  0.9647338554987213,\n",
              "  0.9659526854219949,\n",
              "  0.9657328964194374,\n",
              "  0.9692495204603581,\n",
              "  0.9697690217391305,\n",
              "  0.9702485613810742,\n",
              "  0.9695692135549873,\n",
              "  0.9725263746803069,\n",
              "  0.973525415601023,\n",
              "  0.9717471227621484,\n",
              "  0.9721067774936062,\n",
              "  0.9753037084398977,\n",
              "  0.974704283887468,\n",
              "  0.9765425191815856,\n",
              "  0.976622442455243,\n",
              "  0.9748441496163683,\n",
              "  0.9774216751918159,\n",
              "  0.9780810421994884,\n",
              "  0.977121962915601,\n",
              "  0.9785206202046036,\n",
              "  0.9760230179028133,\n",
              "  0.981218030690537,\n",
              "  0.9789002557544757,\n",
              "  0.9792399296675192,\n",
              "  0.9787603900255755,\n",
              "  0.9827965153452686,\n",
              "  0.9795796035805626,\n",
              "  0.9814977621483376,\n",
              "  0.9824168797953964,\n",
              "  0.9798593350383632,\n",
              "  0.9826166879795396,\n",
              "  0.9820971867007673,\n",
              "  0.9808983375959079,\n",
              "  0.9837356138107417,\n",
              "  0.984494884910486,\n",
              "  0.9831561700767263,\n",
              "  0.9821171675191815,\n",
              "  0.984315057544757,\n",
              "  0.9843350383631714,\n",
              "  0.9838554987212276,\n",
              "  0.9845947890025576,\n",
              "  0.9850543478260869,\n",
              "  0.9845748081841432,\n",
              "  0.9854739450127877,\n",
              "  0.9843550191815856,\n",
              "  0.9848545396419437,\n",
              "  0.9862731777493606,\n",
              "  0.9866128516624041,\n",
              "  0.9858136189258312,\n",
              "  0.98599344629156,\n",
              "  0.9853940217391305,\n",
              "  0.985553868286445,\n",
              "  0.9870724104859335,\n",
              "  0.9867127557544757,\n",
              "  0.9850943094629157,\n",
              "  0.9879315856777494,\n",
              "  0.988650895140665,\n",
              "  0.9871323529411765,\n",
              "  0.9862731777493606,\n",
              "  0.9871723145780051,\n",
              "  0.9869525255754475,\n",
              "  0.9874520460358056,\n",
              "  0.9882512787723785,\n",
              "  0.9864530051150895,\n",
              "  0.9881913363171355,\n",
              "  0.9877917199488491,\n",
              "  0.987412084398977,\n",
              "  0.9878516624040921,\n",
              "  0.9889705882352942,\n",
              "  0.9890105498721228,\n",
              "  0.987352141943734,\n",
              "  0.9887507992327366,\n",
              "  0.9892303388746803,\n",
              "  0.9877917199488491,\n",
              "  0.9893502237851662,\n",
              "  0.9880914322250639,\n",
              "  0.989150415601023,\n",
              "  0.9898097826086957,\n",
              "  0.9888507033248082,\n",
              "  0.9890505115089514,\n",
              "  0.9883911445012787,\n",
              "  0.9895500319693095,\n",
              "  0.9891903772378516,\n",
              "  0.9903093030690537,\n",
              "  0.9888706841432225,\n",
              "  0.9897498401534527,\n",
              "  0.9901494565217391,\n",
              "  0.9899096867007673,\n",
              "  0.989210358056266,\n",
              "  0.9904891304347826,\n",
              "  0.9904891304347826,\n",
              "  0.9907888427109974,\n",
              "  0.9902493606138107,\n",
              "  0.9890105498721228,\n",
              "  0.9910485933503836,\n",
              "  0.9904891304347826,\n",
              "  0.9915880754475703,\n",
              "  0.9901494565217391,\n",
              "  0.9902093989769821,\n",
              "  0.9902293797953964,\n",
              "  0.9912084398976982,\n",
              "  0.9899096867007673,\n",
              "  0.9901294757033248,\n",
              "  0.9912084398976982,\n",
              "  0.9910485933503836,\n",
              "  0.9913682864450127,\n",
              "  0.9911285166240409,\n",
              "  0.9909087276214834,\n",
              "  0.9914482097186701,\n",
              "  0.9912683823529411,\n",
              "  0.9927070012787724,\n",
              "  0.9907888427109974,\n",
              "  0.9911285166240409,\n",
              "  0.9916879795396419,\n",
              "  0.9894301470588235,\n",
              "  0.990888746803069,\n",
              "  0.991568094629156,\n",
              "  0.9911285166240409,\n",
              "  0.9912484015345269,\n",
              "  0.9920276534526854,\n",
              "  0.9911884590792839,\n",
              "  0.9908088235294118,\n",
              "  0.9919277493606138,\n",
              "  0.992806905370844,\n",
              "  0.9908687659846548,\n",
              "  0.9909087276214834,\n",
              "  0.9924872122762148,\n",
              "  0.992307384910486,\n",
              "  0.9925871163682864,\n",
              "  0.9921875,\n",
              "  0.9916480179028133,\n",
              "  0.990828804347826,\n",
              "  0.9926470588235294,\n",
              "  0.9917878836317136,\n",
              "  0.9918877877237852,\n",
              "  0.9920476342710998,\n",
              "  0.9932864450127877,\n",
              "  0.9919077685421995,\n",
              "  0.9929667519181585,\n",
              "  0.9917878836317136,\n",
              "  0.9918678069053708,\n",
              "  0.9926870204603581,\n",
              "  0.9925871163682864,\n",
              "  0.9917878836317136,\n",
              "  0.992806905370844,\n",
              "  0.9927070012787724,\n",
              "  0.9929267902813299,\n",
              "  0.9921075767263428,\n",
              "  0.9927269820971867,\n",
              "  0.9925071930946292,\n",
              "  0.9926070971867008,\n",
              "  0.9923673273657289,\n",
              "  0.9937060421994884,\n",
              "  0.9917479219948849,\n",
              "  0.9919077685421995,\n",
              "  0.9937060421994884,\n",
              "  0.9928668478260869,\n",
              "  0.9932265025575447,\n",
              "  0.9922274616368286,\n",
              "  0.9921475383631714,\n",
              "  0.9930266943734015,\n",
              "  0.9922874040920716,\n",
              "  0.9931665601023018,\n",
              "  0.993486253196931,\n",
              "  0.9928668478260869,\n",
              "  0.9931665601023018,\n",
              "  0.9922674232736572,\n",
              "  0.9924272698209718],\n",
              " 'test_loss': [2.8367242737180867,\n",
              "  2.183150936843483,\n",
              "  1.971450858814701,\n",
              "  1.733079681730574,\n",
              "  1.575959004034662,\n",
              "  1.4761941003951298,\n",
              "  1.4363883847643615,\n",
              "  1.467718907981921,\n",
              "  1.5236264554558285,\n",
              "  1.4095036953118196,\n",
              "  1.3634659048098667,\n",
              "  1.3579507637175785,\n",
              "  1.3622623332746469,\n",
              "  1.3903669528900438,\n",
              "  1.4274903991419798,\n",
              "  1.4901120943628299,\n",
              "  1.5202183089438517,\n",
              "  1.5221052040719683,\n",
              "  1.5523241147114213,\n",
              "  1.6908361482772098,\n",
              "  1.5476766513411406,\n",
              "  1.7134514116937187,\n",
              "  1.692142100850488,\n",
              "  1.6935773924657493,\n",
              "  1.7354588026453734,\n",
              "  1.752320429701714,\n",
              "  1.803772311681395,\n",
              "  1.8354373931125472,\n",
              "  1.8960452075976475,\n",
              "  1.8423703400192746,\n",
              "  1.8051436247339674,\n",
              "  1.9010842700672757,\n",
              "  1.8623068754080754,\n",
              "  1.843025865068861,\n",
              "  1.8772794976355924,\n",
              "  1.8858833499015517,\n",
              "  1.9510544823233489,\n",
              "  2.0304208291564017,\n",
              "  2.010775854253465,\n",
              "  1.9147033243422296,\n",
              "  2.056351360242078,\n",
              "  1.9980817733296923,\n",
              "  2.001470234743349,\n",
              "  2.0225104382083674,\n",
              "  2.0731624175029197,\n",
              "  2.019485867327186,\n",
              "  2.1234449634126795,\n",
              "  2.1045243740081787,\n",
              "  2.1673063783888606,\n",
              "  2.1015401053580507,\n",
              "  2.0993323440005067,\n",
              "  2.1186298355934725,\n",
              "  2.0960904686314286,\n",
              "  2.130021493905669,\n",
              "  2.14228932189334,\n",
              "  2.1456868139801513,\n",
              "  2.1025467473230544,\n",
              "  2.233992599377966,\n",
              "  2.3244153435822503,\n",
              "  2.180928059444306,\n",
              "  2.195363109278831,\n",
              "  2.232769007136108,\n",
              "  2.1441455449268316,\n",
              "  2.1256846827306566,\n",
              "  2.2306988322810763,\n",
              "  2.217516869496388,\n",
              "  2.2023017698792136,\n",
              "  2.2354728689618932,\n",
              "  2.1346277757814733,\n",
              "  2.315295981753404,\n",
              "  2.21989369544254,\n",
              "  2.2927729992350194,\n",
              "  2.2215282643676564,\n",
              "  2.298779637570594,\n",
              "  2.279932091190557,\n",
              "  2.325596228526656,\n",
              "  2.4522388809046167,\n",
              "  2.2785259902856914,\n",
              "  2.272603359951335,\n",
              "  2.353078725231681,\n",
              "  2.3269171676817972,\n",
              "  2.3537664162884853,\n",
              "  2.3700884861551272,\n",
              "  2.318643560834751,\n",
              "  2.2956200716601813,\n",
              "  2.2921672854453896,\n",
              "  2.530243032297511,\n",
              "  2.319103573537936,\n",
              "  2.398658771423777,\n",
              "  2.3890558678633087,\n",
              "  2.4191433548168013,\n",
              "  2.4795412819856293,\n",
              "  2.4477524115781115,\n",
              "  2.4438480526019055,\n",
              "  2.461359106810989,\n",
              "  2.4277548752013285,\n",
              "  2.4554530640316616,\n",
              "  2.5804821017441477,\n",
              "  2.418899221025455,\n",
              "  2.3855137392214147,\n",
              "  2.513035497847636,\n",
              "  2.5448640273634795,\n",
              "  2.4750578897014544,\n",
              "  2.5768157395587603,\n",
              "  2.53536422693046,\n",
              "  2.551268097701346,\n",
              "  2.4512680913232696,\n",
              "  2.58114188415989,\n",
              "  2.4814428652927374,\n",
              "  2.483983090728711,\n",
              "  2.5166728823048294,\n",
              "  2.6378959212333535,\n",
              "  2.505625638992164,\n",
              "  2.523998672795144,\n",
              "  2.568377122757541,\n",
              "  2.580347198589592,\n",
              "  2.4969587553838255,\n",
              "  2.6708820405279754,\n",
              "  2.6030981939309723,\n",
              "  2.6435238730375934,\n",
              "  2.639873307981309,\n",
              "  2.600228869231643,\n",
              "  2.638754190912672,\n",
              "  2.5210639219375173,\n",
              "  2.6081549286083052,\n",
              "  2.5479799547013204,\n",
              "  2.5245561607324394,\n",
              "  2.5729314020484875,\n",
              "  2.521419268125182,\n",
              "  2.5847801949567857,\n",
              "  2.607195876206562,\n",
              "  2.5605095032673733,\n",
              "  2.67761579364728,\n",
              "  2.6154384020787136,\n",
              "  2.6391769996873893,\n",
              "  2.6167527281554643,\n",
              "  2.584235536824366,\n",
              "  2.6440063297368916,\n",
              "  2.642447440487564,\n",
              "  2.6068528404661047,\n",
              "  2.593164909417462,\n",
              "  2.6098621924212027,\n",
              "  2.6258232988369694,\n",
              "  2.7753816136888636,\n",
              "  2.589243197896678,\n",
              "  2.7942690872083045,\n",
              "  2.706379737823632,\n",
              "  2.7388281412185376,\n",
              "  2.7802187341034035,\n",
              "  2.70582080646685,\n",
              "  2.6748692716003224,\n",
              "  2.6908256453313646,\n",
              "  2.6131135589757544,\n",
              "  2.637638440557346,\n",
              "  2.8062224456459095,\n",
              "  2.698474388213674,\n",
              "  2.7360173661238067,\n",
              "  2.742801454416506,\n",
              "  2.6927519117950633,\n",
              "  2.75741914351275,\n",
              "  2.6966174605545725,\n",
              "  2.896102480827623,\n",
              "  2.757315792855184,\n",
              "  2.8612869518577675,\n",
              "  2.749204899095426,\n",
              "  2.7986959575847457,\n",
              "  2.813742972483301,\n",
              "  2.8616892372726634,\n",
              "  2.7857924373286544,\n",
              "  2.7945900236725048,\n",
              "  2.7390288266406695,\n",
              "  2.7895596741111417,\n",
              "  2.8293233062051666,\n",
              "  2.849527314210394,\n",
              "  2.8325525757613454,\n",
              "  2.7775964600265404,\n",
              "  2.768694564035744,\n",
              "  2.804668513832578,\n",
              "  2.8513985378727034,\n",
              "  2.929661955803063,\n",
              "  2.885340173533008,\n",
              "  2.9336771357590985,\n",
              "  2.8528596322247934,\n",
              "  2.8649596382098594,\n",
              "  2.866266398672845,\n",
              "  2.838590894535089,\n",
              "  2.843122992545936,\n",
              "  2.782566419072971,\n",
              "  2.9540810957076444,\n",
              "  2.9610333784370666,\n",
              "  2.8238620545454087,\n",
              "  2.868880366823476,\n",
              "  2.8833412334417843,\n",
              "  2.9112013168395703,\n",
              "  2.8518732619133726,\n",
              "  2.8911394154190257,\n",
              "  2.9405760491729542,\n",
              "  2.87976847180895,\n",
              "  2.857512471022879,\n",
              "  2.892132830467953],\n",
              " 'test_acc': [0.31498805732484075,\n",
              "  0.4309315286624204,\n",
              "  0.49442675159235666,\n",
              "  0.5365246815286624,\n",
              "  0.5692675159235668,\n",
              "  0.5878781847133758,\n",
              "  0.6064888535031847,\n",
              "  0.6123606687898089,\n",
              "  0.6057921974522293,\n",
              "  0.6408240445859873,\n",
              "  0.6397292993630573,\n",
              "  0.6568471337579618,\n",
              "  0.6597332802547771,\n",
              "  0.6642117834394905,\n",
              "  0.6688893312101911,\n",
              "  0.6658041401273885,\n",
              "  0.6706807324840764,\n",
              "  0.6700835987261147,\n",
              "  0.6769506369426752,\n",
              "  0.6651074840764332,\n",
              "  0.6822253184713376,\n",
              "  0.6669984076433121,\n",
              "  0.6710788216560509,\n",
              "  0.6759554140127388,\n",
              "  0.6762539808917197,\n",
              "  0.679140127388535,\n",
              "  0.6743630573248408,\n",
              "  0.6790406050955414,\n",
              "  0.6714769108280255,\n",
              "  0.6723726114649682,\n",
              "  0.6827229299363057,\n",
              "  0.6741640127388535,\n",
              "  0.6800358280254777,\n",
              "  0.682921974522293,\n",
              "  0.6803343949044586,\n",
              "  0.6831210191082803,\n",
              "  0.6827229299363057,\n",
              "  0.67078025477707,\n",
              "  0.6817277070063694,\n",
              "  0.6798367834394905,\n",
              "  0.6770501592356688,\n",
              "  0.6799363057324841,\n",
              "  0.6855095541401274,\n",
              "  0.6827229299363057,\n",
              "  0.6914808917197452,\n",
              "  0.6853105095541401,\n",
              "  0.6853105095541401,\n",
              "  0.6891918789808917,\n",
              "  0.6796377388535032,\n",
              "  0.692078025477707,\n",
              "  0.6900875796178344,\n",
              "  0.683718152866242,\n",
              "  0.6902866242038217,\n",
              "  0.6845143312101911,\n",
              "  0.6887937898089171,\n",
              "  0.692078025477707,\n",
              "  0.6917794585987261,\n",
              "  0.6799363057324841,\n",
              "  0.6743630573248408,\n",
              "  0.6918789808917197,\n",
              "  0.6873009554140127,\n",
              "  0.683718152866242,\n",
              "  0.6886942675159236,\n",
              "  0.6950636942675159,\n",
              "  0.6943670382165605,\n",
              "  0.6934713375796179,\n",
              "  0.6913813694267515,\n",
              "  0.6891918789808917,\n",
              "  0.6949641719745223,\n",
              "  0.6777468152866242,\n",
              "  0.6922770700636943,\n",
              "  0.6854100318471338,\n",
              "  0.6951632165605095,\n",
              "  0.6900875796178344,\n",
              "  0.6897890127388535,\n",
              "  0.6887937898089171,\n",
              "  0.678343949044586,\n",
              "  0.6867038216560509,\n",
              "  0.6926751592356688,\n",
              "  0.6903861464968153,\n",
              "  0.6891918789808917,\n",
              "  0.6911823248407644,\n",
              "  0.6891918789808917,\n",
              "  0.6903861464968153,\n",
              "  0.6879976114649682,\n",
              "  0.694765127388535,\n",
              "  0.6759554140127388,\n",
              "  0.6905851910828026,\n",
              "  0.6854100318471338,\n",
              "  0.690187101910828,\n",
              "  0.6940684713375797,\n",
              "  0.6866042993630573,\n",
              "  0.6916799363057324,\n",
              "  0.6900875796178344,\n",
              "  0.6954617834394905,\n",
              "  0.6944665605095541,\n",
              "  0.6889928343949044,\n",
              "  0.6795382165605095,\n",
              "  0.6908837579617835,\n",
              "  0.6927746815286624,\n",
              "  0.6825238853503185,\n",
              "  0.6867038216560509,\n",
              "  0.6907842356687898,\n",
              "  0.6870023885350318,\n",
              "  0.6889928343949044,\n",
              "  0.6913813694267515,\n",
              "  0.6925756369426752,\n",
              "  0.689390923566879,\n",
              "  0.6996417197452229,\n",
              "  0.6962579617834395,\n",
              "  0.694765127388535,\n",
              "  0.6918789808917197,\n",
              "  0.6962579617834395,\n",
              "  0.6903861464968153,\n",
              "  0.6932722929936306,\n",
              "  0.6903861464968153,\n",
              "  0.6972531847133758,\n",
              "  0.695859872611465,\n",
              "  0.6878980891719745,\n",
              "  0.693172770700637,\n",
              "  0.693968949044586,\n",
              "  0.6981488853503185,\n",
              "  0.6924761146496815,\n",
              "  0.6989450636942676,\n",
              "  0.6949641719745223,\n",
              "  0.6984474522292994,\n",
              "  0.6945660828025477,\n",
              "  0.6965565286624203,\n",
              "  0.7036226114649682,\n",
              "  0.698546974522293,\n",
              "  0.6959593949044586,\n",
              "  0.6981488853503185,\n",
              "  0.693968949044586,\n",
              "  0.7002388535031847,\n",
              "  0.6991441082802548,\n",
              "  0.6964570063694268,\n",
              "  0.7018312101910829,\n",
              "  0.6984474522292994,\n",
              "  0.7001393312101911,\n",
              "  0.6945660828025477,\n",
              "  0.6964570063694268,\n",
              "  0.6948646496815286,\n",
              "  0.6977507961783439,\n",
              "  0.6949641719745223,\n",
              "  0.703125,\n",
              "  0.6914808917197452,\n",
              "  0.6957603503184714,\n",
              "  0.6916799363057324,\n",
              "  0.6921775477707006,\n",
              "  0.6956608280254777,\n",
              "  0.7015326433121019,\n",
              "  0.6987460191082803,\n",
              "  0.6996417197452229,\n",
              "  0.7008359872611465,\n",
              "  0.6928742038216561,\n",
              "  0.7048168789808917,\n",
              "  0.6946656050955414,\n",
              "  0.6908837579617835,\n",
              "  0.6980493630573248,\n",
              "  0.6979498407643312,\n",
              "  0.6987460191082803,\n",
              "  0.6968550955414012,\n",
              "  0.6961584394904459,\n",
              "  0.6938694267515924,\n",
              "  0.7010350318471338,\n",
              "  0.7038216560509554,\n",
              "  0.6988455414012739,\n",
              "  0.6917794585987261,\n",
              "  0.6902866242038217,\n",
              "  0.6986464968152867,\n",
              "  0.6984474522292994,\n",
              "  0.6952627388535032,\n",
              "  0.6981488853503185,\n",
              "  0.6952627388535032,\n",
              "  0.6949641719745223,\n",
              "  0.6980493630573248,\n",
              "  0.6996417197452229,\n",
              "  0.6983479299363057,\n",
              "  0.6977507961783439,\n",
              "  0.6906847133757962,\n",
              "  0.6953622611464968,\n",
              "  0.6932722929936306,\n",
              "  0.6984474522292994,\n",
              "  0.70421974522293,\n",
              "  0.6984474522292994,\n",
              "  0.695859872611465,\n",
              "  0.7018312101910829,\n",
              "  0.70421974522293,\n",
              "  0.6968550955414012,\n",
              "  0.6905851910828026,\n",
              "  0.6971536624203821,\n",
              "  0.7025278662420382,\n",
              "  0.6990445859872612,\n",
              "  0.6995421974522293,\n",
              "  0.7008359872611465,\n",
              "  0.70421974522293,\n",
              "  0.6979498407643312,\n",
              "  0.7016321656050956,\n",
              "  0.6995421974522293,\n",
              "  0.6979498407643312]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train(model=model,\n",
        "          train_dataloader=train_loader,\n",
        "          test_dataloader=test_loader,\n",
        "          optimizer=torch.optim.Adam(params=model.parameters(), lr=0.0001),\n",
        "          loss_fn=F.cross_entropy,\n",
        "          epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl1XtaISFfAw"
      },
      "source": [
        "### Cuvanje modela"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1g8ltRfS6ZMn"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'models/model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl8LkZaXFi4l"
      },
      "source": [
        "### Ucitavanje modela"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "123Sv2ZCbywV"
      },
      "outputs": [],
      "source": [
        "model_normal = Net(3, 100).to(device)\n",
        "model_normal.load_state_dict(torch.load('models/model.pt'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca20d5fc081b4276ba07b599e3c2bc24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1d732c0e0a143888a2ee9463a8c9d71",
              "IPY_MODEL_3e267d1f9c3f40f3a9fcfebbbbe61cff",
              "IPY_MODEL_1960c8d560b9420994f791226ba5cf44"
            ],
            "layout": "IPY_MODEL_abddeae2eaeb4a6da20d6dd771558996"
          }
        },
        "f1d732c0e0a143888a2ee9463a8c9d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56ab8c2dd7a94a0b80d4907e1cc4cbdb",
            "placeholder": "​",
            "style": "IPY_MODEL_ae9ddc230eb243edb04133209904351b",
            "value": "100%"
          }
        },
        "3e267d1f9c3f40f3a9fcfebbbbe61cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb55d150c3334bb9a7d429309fafa922",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_443ba28d72bc4601b1d51867ca444603",
            "value": 200
          }
        },
        "1960c8d560b9420994f791226ba5cf44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf205a682c5844e9939d2eed1d334459",
            "placeholder": "​",
            "style": "IPY_MODEL_bd7d2f4351bf461581e22025aaccde38",
            "value": " 200/200 [37:37&lt;00:00, 11.31s/it]"
          }
        },
        "abddeae2eaeb4a6da20d6dd771558996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ab8c2dd7a94a0b80d4907e1cc4cbdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae9ddc230eb243edb04133209904351b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb55d150c3334bb9a7d429309fafa922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "443ba28d72bc4601b1d51867ca444603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf205a682c5844e9939d2eed1d334459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd7d2f4351bf461581e22025aaccde38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}